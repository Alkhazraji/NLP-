{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Bag-of-Words (BoW) Sentiment Classifier\n",
    "\n",
    "This notebook is using the same data and supporting code as the rule-based classifier.\n",
    "Please see that notebook for details on the data.\n",
    "\n",
    "Unlike the rule-based classifier, this BoW classifier will use automatically extracted features and _learn_ the weights on these features.\n",
    "Our features will be specifically be count vectors where each index refers to the number of a given word found in the input string.\n",
    "This is referred to of as a \"bag of words\" because it's like throwing all of the words into a bag and counting them -- while this method is simple, the main disadvantage is that we lose all structural information present in the sentence(s).\n",
    "We can then use our training data to learn weights between the individual words and our positive, negative, and neutral labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading\n",
    "\n",
    "Read in the data from the training and dev (or finally test) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_XY_data(filename):\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            label, text = line.strip().split(' ||| ')\n",
    "            X_data.append(text)\n",
    "            Y_data.append(int(label))\n",
    "    return X_data, Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8544"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train = read_XY_data('../data/sst-sentiment-text-threeclass/train.txt')\n",
    "X_test, Y_test = read_XY_data('../data/sst-sentiment-text-threeclass/dev.txt')\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(datum):\n",
    "    # Split string into words\n",
    "    return datum.split(\" \")\n",
    "\n",
    "def build_feature_map(X):\n",
    "    # We need to assign an index to each word in order to build the count vector.\n",
    "    # We start by gathering a set of all word types in the training data.\n",
    "    word_types = set()\n",
    "    for datum in X:\n",
    "        for word in tokenize(datum):\n",
    "            word_types.add(word)\n",
    "    # Create a dictionary keyed by word mapping it to an index\n",
    "    return {word: idx for idx, word in enumerate(word_types)}\n",
    "            \n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "def extract_features(word_to_idx, X):\n",
    "    # We are using a sparse matrix from scipy to avoid creating an 8000 x 18000 matrix\n",
    "    features = dok_matrix((len(X), len(word_to_idx)))\n",
    "    for i in range(len(X)):\n",
    "        for word in tokenize(X[i]):\n",
    "            if word in word_to_idx:\n",
    "                # Increment the word count if it is present in the map.\n",
    "                # Unknown words are discarded because we would not have\n",
    "                # a learned weight for them anyway.\n",
    "                features[i, word_to_idx[word]] += 1\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"TAs'\": 0, '<unk>': 1, 'When': 2, 'office': 3, 'are': 4, 'How': 5, 'hard': 6, 'the': 7, 'homework': 8, '?': 9, 'due': 10, 'hours': 11, 'is': 12}\n",
      "\n",
      "  (0, 2)\t1.0\n",
      "  (0, 12)\t1.0\n",
      "  (0, 7)\t1.0\n",
      "  (0, 8)\t1.0\n",
      "  (0, 10)\t1.0\n",
      "  (0, 9)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (1, 4)\t1.0\n",
      "  (1, 7)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 3)\t1.0\n",
      "  (1, 11)\t1.0\n",
      "  (1, 9)\t1.0\n",
      "  (2, 5)\t1.0\n",
      "  (2, 6)\t1.0\n",
      "  (2, 12)\t1.0\n",
      "  (2, 7)\t1.0\n",
      "  (2, 8)\t1.0\n",
      "  (2, 9)\t1.0\n"
     ]
    }
   ],
   "source": [
    "sample_data = [\n",
    "    \"When is the homework due ?\",\n",
    "    \"When are the TAs' office hours ?\",\n",
    "    \"How hard is the homework ?\",\n",
    "]\n",
    "\n",
    "word_to_idx = build_feature_map(sample_data)\n",
    "print(word_to_idx)\n",
    "print()\n",
    "\n",
    "features = extract_features(word_to_idx, sample_data)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the feature extractor on the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique word types in X_train: 18281\n",
      "Sample words:\n",
      "['tinseltown', 'naturalism', 'understated', 'sports', 'boundless', 'factory', 'oppositions', 'suffer', 'original', 'Alternates', 'self-glorified', 'moan', 'strays', 'Berkley', '8-10', 'oppressively', 'alien', 'Alone', 'reaches', 'richer']\n"
     ]
    }
   ],
   "source": [
    "# Build the map based on the training data\n",
    "word_to_idx = build_feature_map(X_train)\n",
    "\n",
    "print(f\"Unique word types in X_train: {len(word_to_idx)}\")\n",
    "print(\"Sample words:\")\n",
    "print(list(word_to_idx.keys())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9701544943820225\n",
      "0.5967302452316077\n"
     ]
    }
   ],
   "source": [
    "# Convert our strings into count vectors\n",
    "X_train_vec = extract_features(word_to_idx, X_train)\n",
    "X_test_vec = extract_features(word_to_idx, X_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(tol=1e1)\n",
    "classifier.fit(X_train_vec, Y_train)\n",
    "print(classifier.score(X_train_vec, Y_train))\n",
    "print(classifier.score(X_test_vec, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
